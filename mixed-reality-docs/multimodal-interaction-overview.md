---
title: Multimodal Interaction Overview
description: Overview of the multimodel interaction
author: Nick, shengkait
ms.author: shengkait
ms.date: 04/11/2019
ms.topic: article
keywords: Mixed Reality, Gaze, gaze targeting, interaction, design
---

# Multimodal Interaction Overview
The philosophy of simple, instinctual interactions is woven throughout the Microsoft Mixed Reality platform, from hardware to software. 

These instinctual interactions bring all major input technologies, including inside out tracking, spatial mapping, hand tracking, eye tracking, and natural language together into seamless multimodal interaction models.  Based on our research, designing and developing multimodally, instead of based on specific inputs, is critical to instinctive experiences. 

Moreover, interaction across MSMR products is naturally aligned across device types.  For example, far interaction with 6DOF controllers on Windows Immersive headsets is compatible with far interaction using your hands on HL2.  Not only is this convenient for developers and designers, but it feels natural to end users. 

We've structures our guidance to reflect this philosophy: that the most instinctual interaction models are multimodal, and that interaction is consistent across device types and whether users are using controllers or not.

Still, to design and develop multimodally, it is important to understand the capabilities of each input.

## Interaction Models
Based on our research and work with customers to date, we've discovered three primary interaction models that suit the majority of Mixed Reality experiences.  Think of these interaction models as the user's mental model for completing their flows. 

<table>
    <colgroup>
    <col width="33%" />
    <col width="33%" />
    <col width="33%" />
    </colgroup>
    <tr>
        <td><strong>Hands and Tools</strong></td>
        <td><strong>Hands-Free</strong></td>
        <td><strong>Gaze and Commit</strong></td>
    </tr>
    
</table>
## Input Models
